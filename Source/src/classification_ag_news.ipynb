{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SON' from 'bson' (c:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\bson\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m encode, decode\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MongoClient\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Code\n",
      "File \u001b[1;32mc:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\pymongo\\__init__.py:91\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _csot\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, get_version_string, version_tuple\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MAX_SUPPORTED_WIRE_VERSION, MIN_SUPPORTED_WIRE_VERSION, has_c\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcursor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CursorType\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     94\u001b[0m     DeleteMany,\n\u001b[0;32m     95\u001b[0m     DeleteOne,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     UpdateOne,\n\u001b[0;32m    101\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\pymongo\\common.py:39\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     25\u001b[0m     Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     overload,\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unquote_plus\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbson\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SON\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbinary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UuidRepresentation\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodec_options\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CodecOptions, DatetimeConversion, TypeRegistry\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SON' from 'bson' (c:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\bson\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from codecs import encode, decode\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "from bson.code import Code\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import aggregation_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client.ag_news_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'ag_news_classification', 'config', 'local', 'test']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TotalCounts', 'WordCounts', 'Results', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "print(db.list_collection_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Input Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_no_hidden(path):\n",
    "    return glob.glob(os.path.join(path, '*'))\n",
    "\n",
    "def create_content(filename):\n",
    "    text = []\n",
    "    with open(filename, 'r', encoding='latin2') as f:\n",
    "        for line in f:\n",
    "            for word in line.strip().split():\n",
    "                text.append(word)\n",
    "    return text\n",
    "\n",
    "\n",
    "def create_preprocessed_content(filename):\n",
    "    \"\"\"\n",
    "    :param filename:\n",
    "    :return: preprocessed text (string)\n",
    "    \"\"\"\n",
    "\n",
    "    text= []\n",
    "    with open(filename, 'r', encoding= 'latin2') as f:\n",
    "        for line in f: \n",
    "            new_line = text_to_words(line)\n",
    "            for word in new_line.strip().split():\n",
    "                text.append(word)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def text_to_words(raw_text):\n",
    "    \"\"\"\n",
    "    :param raw text:\n",
    "    :return: string of words with removed stop words\n",
    "    \"\"\"\n",
    "    # Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    # Join the words back into one string separated by space,\n",
    "    # and return the result.\n",
    "    return \" \".join(meaningful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_collection(path= \"../ag_news_data/train\"):  # create test collection with 2 class : X and Y\n",
    "    \n",
    "    db.train.drop()\n",
    "    for file in listdir_no_hidden(path):\n",
    "        my_dict = {}\n",
    "        class_x = 0\n",
    "        class_y = 0\n",
    "        my_dict['content'] = create_preprocessed_content(file)\n",
    "        # my_dict['content'] = create_preprocessed_content(file)\n",
    "        if \"fortnow\" in file:\n",
    "            class_x = 1\n",
    "        else:\n",
    "            class_y = 1\n",
    "        my_dict['classX'] = class_x  #need to turn into 4 classes\n",
    "        my_dict['classY'] = class_y\n",
    "        db.train.insert_one(my_dict)\n",
    "\n",
    "\n",
    "def create_test_collection(path= \"../ag_news_data/test\"):           # create test collection with 2 class : X and Y\n",
    "\n",
    "    db.test.drop()\n",
    "    for file in listdir_no_hidden(path):\n",
    "        my_dict = {}\n",
    "        class_x = 0\n",
    "        class_y = 0\n",
    "        my_dict['content'] = create_preprocessed_content(file)\n",
    "        # my_dict['content'] = create_preprocessed_content(file)\n",
    "        if \"fortnow\" in file:\n",
    "            class_x = 1\n",
    "        else:\n",
    "            class_y = 1\n",
    "        my_dict['classX'] = class_x #need to turn into 4 classes\n",
    "        my_dict['classY'] = class_y\n",
    "        my_dict['predclassX'] = 0\n",
    "        my_dict['predclassY'] = 0\n",
    "        db.test.insert_one(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_collection_dataframe(path= \"../ag_news_data/train.csv\"):  # create test collection with 2 class : X and Y\n",
    "    df = pd.read_csv(path)\n",
    "    db.train.drop()\n",
    "    res = []\n",
    "    for idx, row in df.iterrows():\n",
    "        my_dict = {}\n",
    "        classes = [0 for i in range(4)]\n",
    "        my_dict['content'] = text_to_words(row[\"text\"]).strip().split()\n",
    "        label = row[\"label\"]\n",
    "        classes[label] = 1\n",
    "        my_dict['class0'] = classes[0]\n",
    "        my_dict['class1'] = classes[1]\n",
    "        my_dict['class2'] = classes[2]\n",
    "        my_dict['class3'] = classes[3]\n",
    "        res.append(my_dict)\n",
    "    db.train.insert_many(res)\n",
    "\n",
    "def create_test_collection_dataframe(path= \"../ag_news_data/test.csv\"):           # create test collection with 2 class : X and Y\n",
    "    df = pd.read_csv(path)\n",
    "    db.test.drop()\n",
    "    res = []\n",
    "    for idx, row in df.iterrows():\n",
    "        my_dict = {}\n",
    "        classes = [0 for i in range(4)]\n",
    "        my_dict['content'] = text_to_words(row[\"text\"]).strip().split()\n",
    "        label = row[\"label\"]\n",
    "        classes[label] = 1\n",
    "        my_dict['class0'], my_dict['predclass0'] = classes[0],0 \n",
    "        my_dict['class1'], my_dict['predclass1'] = classes[1],0\n",
    "        my_dict['class2'], my_dict['predclass2'] = classes[2],0\n",
    "        my_dict['class3'], my_dict['predclass3'] = classes[3],0\n",
    "        res.append(my_dict)\n",
    "    db.test.insert_many(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce for Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce_total_counts():\n",
    "    db.TotalCounts.drop()\n",
    "    pipeline_one = aggregation_pipeline.get_total_counts_pipeline()\n",
    "    db.train.aggregate(pipeline_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce_word_counts():\n",
    "    db.WordCounts.drop()\n",
    "    pipeline_two = aggregation_pipeline.get_word_counts_pipeline()\n",
    "\n",
    "    db.train.aggregate(pipeline_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce for Test dataset Classification \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce():\n",
    "    map_reduce_total_counts()\n",
    "    map_reduce_word_counts()\n",
    "    vocabulary = db.TotalCounts.find_one()['V']\n",
    "\n",
    "    class_0 = db.TotalCounts.find_one()['cl0']\n",
    "    class_1 = db.TotalCounts.find_one()['cl1']\n",
    "    class_2 = db.TotalCounts.find_one()['cl2']\n",
    "    class_3 = db.TotalCounts.find_one()['cl3']\n",
    "\n",
    "    docs = db.train.find()\n",
    "    test_docs = db.test.find()\n",
    "    \n",
    "    return vocabulary, class_0, class_1, class_2, class_3, docs, test_docs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../image/naive_bayes.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifier():\n",
    "    ## P(x),  P(C_k)\n",
    "    vocabulary_len, class_0, class_1, class_2, class_3, docs, test_docs = map_reduce()\n",
    "    n = 120000.0\n",
    "    classes_vocab_len = np.array([class_0, class_1, class_2, class_3])\n",
    "    classes_sample_num = np.array([30000.0, 30000.0, 30000.0, 30000.0])\n",
    "    denominator = classes_vocab_len + vocabulary_len    # smooth P(x)\n",
    "    probability_class = classes_sample_num / n        # P(C_k)\n",
    "    \n",
    "    # P(C_k) * P(x|C_k)\n",
    "    for doc in test_docs:\n",
    "\n",
    "        label_prob = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "        for word in doc['content']:\n",
    "            dict_two = db.WordCounts.find_one({\"_id\": word})\n",
    "            if dict_two is not None:\n",
    "                word_class = [dict_two['value']['class0'], dict_two['value']['class1'], dict_two['value']['class2'], dict_two['value']['class3']]\n",
    "            else:\n",
    "                word_class = [0.0, 0.0, 0.0, 0.0]\n",
    "            label_prob += np.log10((np.array(word_class) + 1.0) / denominator)  # P(x|C_k)\n",
    "            \n",
    "        label_prob += np.log10(probability_class)  # P(C_k) * P(x|C_k)\n",
    "        \n",
    "        label = np.argmax(label_prob)  # pred label\n",
    "        if label == 0:\n",
    "            db.test.update_one(doc, {'$set': {'predClass0': 1, 'predClass1': 0, 'predClass2': 0, 'predClass3': 0}})\n",
    "        elif label == 1:\n",
    "            db.test.update_one(doc, {'$set': {'predClass0': 0, 'predClass1': 1, 'predClass2': 0, 'predClass3': 0}})\n",
    "        elif label == 2:\n",
    "            db.test.update_one(doc, {'$set': {'predClass0': 0, 'predClass1': 0, 'predClass2': 1, 'predClass3': 0}})\n",
    "        else:\n",
    "            db.test.update_one(doc, {'$set': {'predClass0': 0, 'predClass1': 0, 'predClass2': 0, 'predClass3': 1}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_result():\n",
    "    naive_bayes_classifier()\n",
    "    \n",
    "    pipeline = aggregation_pipeline.get_statistics_result_pipeline()\n",
    "    \n",
    "    db.test.aggregate(pipeline)\n",
    "    \n",
    "    calculated_confusion_matrix = db.Results.find_one()['value']\n",
    "    return calculated_confusion_matrix\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_test_collection_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_collection_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class0_true': 7226.0, 'class0_false': 374.0, 'class1_true': 7468.0, 'class1_false': 132.0, 'class2_true': 7078.0, 'class2_false': 522.0, 'class3_true': 7130.0, 'class3_false': 470.0}\n"
     ]
    }
   ],
   "source": [
    "a = statistics_result()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text: str):\n",
    "    # get necessary statistics data\n",
    "    vocabulary_len = db.TotalCounts.find_one()['V']\n",
    "\n",
    "    n = 120000.0\n",
    "    class_0 = db.TotalCounts.find_one()['cl0']\n",
    "    class_1 = db.TotalCounts.find_one()['cl1']\n",
    "    class_2 = db.TotalCounts.find_one()['cl2']\n",
    "    class_3 = db.TotalCounts.find_one()['cl3']\n",
    "    \n",
    "    classes_vocab_len = np.array([class_0, class_1, class_2, class_3])\n",
    "    classes_sample_num = np.array([30000.0, 30000.0, 30000.0, 30000.0])\n",
    "    denominator = classes_vocab_len + vocabulary_len    # smooth P(x)\n",
    "    probability_class = classes_sample_num / n        # P(C_k)\n",
    "    \n",
    "    \n",
    "    # Process input text\n",
    "    words = text_to_words(text).strip().split()\n",
    "    # P(C_k) * P(x|C_k)\n",
    "    label_prob = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "    for word in words:\n",
    "        dict_two = db.WordCounts.find_one({\"_id\": word})\n",
    "        if dict_two is not None:\n",
    "            word_class = [dict_two['value']['class0'], dict_two['value']['class1'], dict_two['value']['class2'], dict_two['value']['class3']]\n",
    "        else:\n",
    "            word_class = [0.0, 0.0, 0.0, 0.0]\n",
    "        label_prob += np.log10((np.array(word_class) + 1.0) / denominator)  # P(x|C_k)\n",
    "        \n",
    "    label_prob += np.log10(probability_class)  # P(C_k) * P(x|C_k)\n",
    "    \n",
    "    label = np.argmax(label_prob)  # pred label\n",
    "    index_to_label = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci_Tech\"}\n",
    "    return index_to_label[label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Comets, Asteroids and Planets around a Nearby Star (SPACE.com) SPACE.com - A nearby star thought to harbor comets and asteroids now appears to be home to planets, too. The presumed worlds are smaller than Jupiter and could be as tiny as Pluto, new observations suggest.'\"classification copy.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci_Tech\n"
     ]
    }
   ],
   "source": [
    "print(inference(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
