{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from codecs import encode, decode\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "from bson.code import Code\n",
    "from nltk.corpus import stopwords\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client.test\n",
    "# total number of documents in train/test set\n",
    "n = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'config', 'local', 'test']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TotalCounts', 'WordCounts', 'train', 'test']\n"
     ]
    }
   ],
   "source": [
    "print(db.list_collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_no_hidden(path):\n",
    "    return glob.glob(os.path.join(path, '*'))\n",
    "\n",
    "\n",
    "# def create_content(filename):\n",
    "\n",
    "#     with open(filename) as f:\n",
    "#         text = []\n",
    "#         for line in f:\n",
    "#             for word in line.strip().split():\n",
    "#                 word = decode(word.strip(), 'latin2', 'ignore')\n",
    "#                 text.append(word)\n",
    "#     return text\n",
    "def create_content(filename):\n",
    "    text = []\n",
    "    with open(filename, 'r', encoding='latin2') as f:\n",
    "        for line in f:\n",
    "            for word in line.strip().split():\n",
    "                text.append(word)\n",
    "    return text\n",
    "\n",
    "\n",
    "def create_preprocessed_content(filename):\n",
    "    \"\"\"\n",
    "    :param filename:\n",
    "    :return: preprocessed text (string)\n",
    "    \"\"\"\n",
    "\n",
    "    # with open(filename) as f:\n",
    "    #     text = []\n",
    "    #     for line in f:\n",
    "    #         new_line = text_to_words(line)\n",
    "    #         for word in new_line.strip().split():\n",
    "    #             word = decode(word.strip(), 'latin2', 'ignore')\n",
    "    #             text.append(word)\n",
    "    text= []\n",
    "    with open(filename, 'r', encoding= 'latin2') as f:\n",
    "        for line in f: \n",
    "            new_line = text_to_words(line)\n",
    "            for word in new_line.strip().split():\n",
    "                text.append(word)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def text_to_words(raw_text):\n",
    "    \"\"\"\n",
    "    :param raw text:\n",
    "    :return: string of words with removed stop words\n",
    "    \"\"\"\n",
    "    # Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    # Join the words back into one string separated by space,\n",
    "    # and return the result.\n",
    "    return \" \".join(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_collection():  # create test collection with 2 class : X and Y\n",
    "    \n",
    "    db.train.drop()\n",
    "    for file in listdir_no_hidden(\"../data/train\"):\n",
    "        my_dict = {}\n",
    "        class_x = 0\n",
    "        class_y = 0\n",
    "        my_dict['content'] = create_preprocessed_content(file)\n",
    "        # my_dict['content'] = create_preprocessed_content(file)\n",
    "        if \"fortnow\" in file:\n",
    "            class_x = 1\n",
    "        else:\n",
    "            class_y = 1\n",
    "        my_dict['classX'] = class_x  #need to turn into 4 classes\n",
    "        my_dict['classY'] = class_y\n",
    "        db.train.insert_one(my_dict)\n",
    "\n",
    "\n",
    "def create_test_collection():           # create test collection with 2 class : X and Y\n",
    "\n",
    "    db.test.drop()\n",
    "    for file in listdir_no_hidden(\"../data/test\"):\n",
    "        my_dict = {}\n",
    "        class_x = 0\n",
    "        class_y = 0\n",
    "        my_dict['content'] = create_preprocessed_content(file)\n",
    "        # my_dict['content'] = create_preprocessed_content(file)\n",
    "        if \"fortnow\" in file:\n",
    "            class_x = 1\n",
    "        else:\n",
    "            class_y = 1\n",
    "        my_dict['classX'] = class_x #need to turn into 4 classes\n",
    "        my_dict['classY'] = class_y\n",
    "        my_dict['predclassX'] = 0\n",
    "        my_dict['predclassY'] = 0\n",
    "        db.test.insert_one(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce_total_counts():\n",
    "    db.TotalCounts.drop()\n",
    "    pipeline_one = [\n",
    "            { #mapper\n",
    "            \"$project\": {\n",
    "                \"clX\": {\n",
    "                    \"$cond\": {\n",
    "                        \"if\": { \"$eq\": [\"$classX\", 1] },\n",
    "                        \"then\": { \"$size\": \"$content\" },\n",
    "                        \"else\": 0\n",
    "                    }\n",
    "                },\n",
    "                \"clY\": {\n",
    "                    \"$cond\": {\n",
    "                        \"if\": { \"$ne\": [\"$classX\", 1] },\n",
    "                        \"then\": { \"$size\": \"$content\" },\n",
    "                        \"else\": 0\n",
    "                    }\n",
    "                },\n",
    "                \"V\": { \"$size\": \"$content\" }\n",
    "            }\n",
    "        },\n",
    "        { #reducer\n",
    "            \"$group\": {\n",
    "                \"_id\": \"doc\",\n",
    "                \"clX\": { \"$sum\": \"$clX\" },\n",
    "                \"clY\": { \"$sum\": \"$clY\" },\n",
    "                \"V\": { \"$sum\": \"$V\" }\n",
    "            }\n",
    "        },\n",
    "        { #output\n",
    "            \"$out\": \"TotalCounts\"\n",
    "        }\n",
    "        ]\n",
    "\n",
    "    db.train.aggregate(pipeline_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce_word_counts():\n",
    "    db.WordCounts.drop()\n",
    "    pipeline_two = [\n",
    "        # Step 1: Unwind the content array to create a document for each word\n",
    "    { \n",
    "        \"$unwind\": \"$content\" \n",
    "    },\n",
    "    \n",
    "    # Step 2: Project the required fields with the exact structure\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"word\": \"$content\",\n",
    "            \"value\": {\n",
    "                \"classX\": \"$classX\",\n",
    "                \"classY\": \"$classY\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Step 3: Group by word (equivalent to the reduce phase)\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$word\",\n",
    "            \"classX\": { \"$sum\": \"$value.classX\" },\n",
    "            \"classY\": { \"$sum\": \"$value.classY\" }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Step 4: Reshape the output to match the original format\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 1,\n",
    "            \"value\": {\n",
    "                \"classX\": \"$classX\",\n",
    "                \"classY\": \"$classY\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Step 5: Write to output collection\n",
    "    {\n",
    "        \"$out\": \"WordCounts\"\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    db.train.aggregate(pipeline_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce():\n",
    "    map_reduce_total_counts()\n",
    "    map_reduce_word_counts()\n",
    "    vocabulary = db.TotalCounts.find_one()['V']\n",
    "\n",
    "    class_x = db.TotalCounts.find_one()['clX']\n",
    "    class_y = db.TotalCounts.find_one()['clY']\n",
    "\n",
    "    docs = db.train.find()\n",
    "    test_docs = db.test.find()\n",
    "    \n",
    "    return vocabulary, class_x, class_y, docs, test_docs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifier():\n",
    "    vocabulary, class_x, class_y, docs, test_docs = map_reduce()\n",
    "    n = 60.0\n",
    "    n_class_x = 30.0\n",
    "    n_class_y = 30.0\n",
    "    denominator_x = class_x + vocabulary\n",
    "    denominator_y = class_y + vocabulary\n",
    "    probability_class_x = math.log10(n_class_x / n)\n",
    "    probability_class_y = math.log10(n_class_y / n)\n",
    "    for doc in test_docs:\n",
    "        sum_x = 0.0\n",
    "        sum_y = 0.0\n",
    "        for word in doc['content']:\n",
    "            dict_two = db.WordCounts.find_one({\"_id\": word})\n",
    "            if dict_two is not None:\n",
    "                word_class_x = dict_two['value']['classX']\n",
    "                word_class_y = dict_two['value']['classY']\n",
    "            else:\n",
    "                word_class_x = 0.0\n",
    "                word_class_y = 0.0\n",
    "            sum_x += math.log10((word_class_x + 1.0) / denominator_x)\n",
    "            sum_y += math.log10((word_class_y + 1.0) / denominator_y)\n",
    "        x = sum_x + probability_class_x\n",
    "        y = sum_y + probability_class_y\n",
    "        if x > y:\n",
    "            db.test.update_one(doc, {'$set': {'predClassX': 1, 'predClassY': 0}})\n",
    "        else:\n",
    "            db.test.find_one_and_update(doc, {'$set': {'predClassX': 0, 'predClassY': 1}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix():\n",
    "    naive_bayes_classifier()\n",
    "    db.test.aggregate([\n",
    "        {\n",
    "        \"$project\": {\n",
    "            \"emit\": {\n",
    "                \"key\": \"doc\",\n",
    "                \"value\": {\n",
    "                    \"a\": {\n",
    "                        \"$cond\": [\n",
    "                            {\"$and\": [\n",
    "                                {\"$eq\": [\"$classX\", 1]},\n",
    "                                {\"$eq\": [\"$predClassX\", 1]}\n",
    "                            ]},\n",
    "                            1,\n",
    "                            0\n",
    "                        ]\n",
    "                    },\n",
    "                    \"b\": {\n",
    "                        \"$cond\": [\n",
    "                            {\"$and\": [\n",
    "                                {\"$eq\": [\"$classX\", 1]},\n",
    "                                {\"$eq\": [\"$predClassX\", 0]}\n",
    "                            ]},\n",
    "                            1,\n",
    "                            0\n",
    "                        ]\n",
    "                    },\n",
    "                    \"c\": {\n",
    "                        \"$cond\": [\n",
    "                            {\"$and\": [\n",
    "                                {\"$eq\": [\"$classX\", 0]},\n",
    "                                {\"$eq\": [\"$predClassX\", 1]}\n",
    "                            ]},\n",
    "                            1,\n",
    "                            0\n",
    "                        ]\n",
    "                    },\n",
    "                    \"d\": {\n",
    "                        \"$cond\": [\n",
    "                            {\"$and\": [\n",
    "                                {\"$eq\": [\"$classX\", 0]},\n",
    "                                {\"$eq\": [\"$predClassX\", 0]}\n",
    "                            ]},\n",
    "                            1,\n",
    "                            0\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    {\n",
    "    \"$group\": {\n",
    "        \"_id\": \"$emit.key\",\n",
    "        \"value\": {\n",
    "            \"$accumulator\": {\n",
    "                \"init\": \"function() { return {a: 0, b: 0, c: 0, d: 0}; }\",\n",
    "                \"accumulate\": \"function(state, value) { return {a: state.a + value.a, b: state.b + value.b, c: state.c + value.c, d: state.d + value.d}; }\",\n",
    "                \"accumulateArgs\": [\"$emit.value\"],\n",
    "                \"merge\": \"function(state1, state2) { return {a: state1.a + state2.a, b: state1.b + state2.b, c: state1.c + state2.c, d: state1.d + state2.d}; }\",\n",
    "                \"lang\": \"js\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        },\n",
    "        {\n",
    "            \"$out\": \"Results\"\n",
    "        }\n",
    "    ])\n",
    "    calculated_confusion_matrix = db.Results.find_one()['value']\n",
    "    return calculated_confusion_matrix\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_test_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': 'raz', 'value': {'classX': 3, 'classY': 0}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.WordCounts.find_one()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 26.0, 'b': 4.0, 'c': 3.0, 'd': 27.0}\n"
     ]
    }
   ],
   "source": [
    "a = confusion_matrix()\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btl-big-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
