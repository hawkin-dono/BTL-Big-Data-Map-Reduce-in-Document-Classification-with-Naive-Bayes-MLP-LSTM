{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from codecs import encode, decode\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "from bson.code import Code\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import aggregation_pipeline\n",
    "import sys \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client.ag_news_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'ag_news_classification', 'config', 'local', 'test']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WordCounts', 'test', 'train', 'Results', 'TotalCounts']\n"
     ]
    }
   ],
   "source": [
    "print(db.list_collection_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Input Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_no_hidden(path):\n",
    "    return glob.glob(os.path.join(path, '*'))\n",
    "\n",
    "def create_content(filename):\n",
    "    text = []\n",
    "    with open(filename, 'r', encoding='latin2') as f:\n",
    "        for line in f:\n",
    "            for word in line.strip().split():\n",
    "                text.append(word)\n",
    "    return text\n",
    "\n",
    "\n",
    "def create_preprocessed_content(filename):\n",
    "    \"\"\"\n",
    "    :param filename:\n",
    "    :return: preprocessed text (string)\n",
    "    \"\"\"\n",
    "\n",
    "    text= []\n",
    "    with open(filename, 'r', encoding= 'latin2') as f:\n",
    "        for line in f: \n",
    "            new_line = text_to_words(line)\n",
    "            for word in new_line.strip().split():\n",
    "                text.append(word)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def text_to_words(raw_text):\n",
    "    \"\"\"\n",
    "    :param raw text:\n",
    "    :return: string of words with removed stop words\n",
    "    \"\"\"\n",
    "    # Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    # Join the words back into one string separated by space,\n",
    "    # and return the result.\n",
    "    return \" \".join(meaningful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listdir_no_hidden(\"../../AllData/ag_news_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_collection(path= \"../../AllData/ag_news_data\"):  # create test collection with 2 class : X and Y\n",
    "    \n",
    "    db.train.drop()\n",
    "    for file in listdir_no_hidden(path):\n",
    "        my_dict = {}\n",
    "        class_x = 0\n",
    "        class_y = 0\n",
    "        my_dict['content'] = create_preprocessed_content(file)\n",
    "        # my_dict['content'] = create_preprocessed_content(file)\n",
    "        if \"fortnow\" in file:\n",
    "            class_x = 1\n",
    "        else:\n",
    "            class_y = 1\n",
    "        my_dict['classX'] = class_x  #need to turn into 4 classes\n",
    "        my_dict['classY'] = class_y\n",
    "        db.train.insert_one(my_dict)\n",
    "\n",
    "\n",
    "def create_test_collection(path= \"../../AllData/ag_news_data\"):           # create test collection with 2 class : X and Y\n",
    "\n",
    "    db.test.drop()\n",
    "    for file in listdir_no_hidden(path):\n",
    "        my_dict = {}\n",
    "        class_x = 0\n",
    "        class_y = 0\n",
    "        my_dict['content'] = create_preprocessed_content(file)\n",
    "        # my_dict['content'] = create_preprocessed_content(file)\n",
    "        if \"fortnow\" in file:\n",
    "            class_x = 1\n",
    "        else:\n",
    "            class_y = 1\n",
    "        my_dict['classX'] = class_x #need to turn into 4 classes\n",
    "        my_dict['classY'] = class_y\n",
    "        my_dict['predclassX'] = 0\n",
    "        my_dict['predclassY'] = 0\n",
    "        db.test.insert_one(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_collection_dataframe(path= \"../../AllData/ag_news_data/train.csv\"):  # create test collection with 2 class : X and Y\n",
    "    df = pd.read_csv(path)\n",
    "    db.train.drop()\n",
    "    res = []\n",
    "    for idx, row in df.iterrows():\n",
    "        my_dict = {}\n",
    "        classes = [0 for i in range(4)]\n",
    "        my_dict['content'] = text_to_words(row[\"text\"]).strip().split()\n",
    "        label = row[\"label\"]\n",
    "        classes[label] = 1\n",
    "        my_dict['class0'] = classes[0]\n",
    "        my_dict['class1'] = classes[1]\n",
    "        my_dict['class2'] = classes[2]\n",
    "        my_dict['class3'] = classes[3]\n",
    "        res.append(my_dict)\n",
    "    db.train.insert_many(res)\n",
    "\n",
    "def create_test_collection_dataframe(path= \"../../AllData/ag_news_data/test.csv\"):           # create test collection with 2 class : X and Y\n",
    "    df = pd.read_csv(path)\n",
    "    db.test.drop()\n",
    "    res = []\n",
    "    for idx, row in df.iterrows():\n",
    "        my_dict = {}\n",
    "        classes = [0 for i in range(4)]\n",
    "        my_dict['content'] = text_to_words(row[\"text\"]).strip().split()\n",
    "        label = row[\"label\"]\n",
    "        classes[label] = 1\n",
    "        my_dict['class0'], my_dict['predclass0'] = classes[0],0 \n",
    "        my_dict['class1'], my_dict['predclass1'] = classes[1],0\n",
    "        my_dict['class2'], my_dict['predclass2'] = classes[2],0\n",
    "        my_dict['class3'], my_dict['predclass3'] = classes[3],0\n",
    "        res.append(my_dict)\n",
    "    db.test.insert_many(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce for Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce_total_counts():\n",
    "    db.TotalCounts.drop()\n",
    "    pipeline_one = aggregation_pipeline.get_total_counts_pipeline()\n",
    "    db.train.aggregate(pipeline_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce_word_counts():\n",
    "    db.WordCounts.drop()\n",
    "    pipeline_two = aggregation_pipeline.get_word_counts_pipeline()\n",
    "\n",
    "    db.train.aggregate(pipeline_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce for Test dataset Classification \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce():\n",
    "    map_reduce_total_counts()\n",
    "    map_reduce_word_counts()\n",
    "    vocabulary = db.TotalCounts.find_one()['V']\n",
    "\n",
    "    class_0 = db.TotalCounts.find_one()['cl0']\n",
    "    class_1 = db.TotalCounts.find_one()['cl1']\n",
    "    class_2 = db.TotalCounts.find_one()['cl2']\n",
    "    class_3 = db.TotalCounts.find_one()['cl3']\n",
    "\n",
    "    docs = db.train.find()\n",
    "    test_docs = db.test.find()\n",
    "    \n",
    "    return vocabulary, class_0, class_1, class_2, class_3, docs, test_docs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../image/naive_bayes.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifier():\n",
    "    ## P(x),  P(C_k)\n",
    "    vocabulary_len, class_0, class_1, class_2, class_3, docs, test_docs = map_reduce()\n",
    "    n = 120000.0\n",
    "    classes_vocab_len = np.array([class_0, class_1, class_2, class_3])\n",
    "    classes_sample_num = np.array([30000.0, 30000.0, 30000.0, 30000.0])\n",
    "    denominator = classes_vocab_len + vocabulary_len    # smooth P(x)\n",
    "    probability_class = classes_sample_num / n        # P(C_k)\n",
    "    \n",
    "    # P(C_k) * P(x|C_k)\n",
    "    for doc in test_docs:\n",
    "\n",
    "        label_prob = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "        for word in doc['content']:\n",
    "            dict_two = db.WordCounts.find_one({\"_id\": word})\n",
    "            if dict_two is not None:\n",
    "                word_class = [dict_two['value']['class0'], dict_two['value']['class1'], dict_two['value']['class2'], dict_two['value']['class3']]\n",
    "            else:\n",
    "                word_class = [0.0, 0.0, 0.0, 0.0]\n",
    "            label_prob += np.log10((np.array(word_class) + 1.0) / denominator)  # P(x|C_k)\n",
    "            \n",
    "        label_prob += np.log10(probability_class)  # P(C_k) * P(x|C_k)\n",
    "        \n",
    "        label = np.argmax(label_prob)  # pred label\n",
    "        if label == 0:\n",
    "            db.test.update_one(doc, {'$set': {'predClass0': 1, 'predClass1': 0, 'predClass2': 0, 'predClass3': 0}})\n",
    "        elif label == 1:\n",
    "            db.test.update_one(doc, {'$set': {'predClass0': 0, 'predClass1': 1, 'predClass2': 0, 'predClass3': 0}})\n",
    "        elif label == 2:\n",
    "            db.test.update_one(doc, {'$set': {'predClass0': 0, 'predClass1': 0, 'predClass2': 1, 'predClass3': 0}})\n",
    "        else:\n",
    "            db.test.update_one(doc, {'$set': {'predClass0': 0, 'predClass1': 0, 'predClass2': 0, 'predClass3': 1}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_result():\n",
    "    naive_bayes_classifier()\n",
    "    \n",
    "    pipeline = aggregation_pipeline.get_statistics_result_pipeline()\n",
    "    \n",
    "    db.test.aggregate(pipeline)\n",
    "    \n",
    "    calculated_confusion_matrix = db.Results.find_one()['value']\n",
    "    return calculated_confusion_matrix\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_test_collection_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_collection_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': 'doc',\n",
       " 'cl0': 792327,\n",
       " 'cl1': 729712,\n",
       " 'cl2': 809174,\n",
       " 'cl3': 772064,\n",
       " 'V': 3103277}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce_total_counts()\n",
    "db.TotalCounts.find_one()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': 'webber',\n",
       " 'value': {'class0': 5, 'class1': 24, 'class2': 0, 'class3': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce_word_counts()\n",
    "db.WordCounts.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 99.28264904022217\n",
      "{'class0_true': 7226.0, 'class0_false': 374.0, 'class1_true': 7468.0, 'class1_false': 132.0, 'class2_true': 7078.0, 'class2_false': 522.0, 'class3_true': 7130.0, 'class3_false': 470.0}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "a = statistics_result()\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start}\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text: str):\n",
    "    # get necessary statistics data\n",
    "    vocabulary_len = db.TotalCounts.find_one()['V']\n",
    "\n",
    "    n = 120000.0\n",
    "    class_0 = db.TotalCounts.find_one()['cl0']\n",
    "    class_1 = db.TotalCounts.find_one()['cl1']\n",
    "    class_2 = db.TotalCounts.find_one()['cl2']\n",
    "    class_3 = db.TotalCounts.find_one()['cl3']\n",
    "    \n",
    "    classes_vocab_len = np.array([class_0, class_1, class_2, class_3])\n",
    "    classes_sample_num = np.array([30000.0, 30000.0, 30000.0, 30000.0])\n",
    "    denominator = classes_vocab_len + vocabulary_len    # smooth P(x)\n",
    "    probability_class = classes_sample_num / n        # P(C_k)\n",
    "    \n",
    "    \n",
    "    # Process input text\n",
    "    words = text_to_words(text).strip().split()\n",
    "    # P(C_k) * P(x|C_k)\n",
    "    label_prob = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "    for word in words:\n",
    "        dict_two = db.WordCounts.find_one({\"_id\": word})\n",
    "        if dict_two is not None:\n",
    "            word_class = [dict_two['value']['class0'], dict_two['value']['class1'], dict_two['value']['class2'], dict_two['value']['class3']]\n",
    "        else:\n",
    "            word_class = [0.0, 0.0, 0.0, 0.0]\n",
    "        label_prob += np.log10((np.array(word_class) + 1.0) / denominator)  # P(x|C_k)\n",
    "        \n",
    "    label_prob += np.log10(probability_class)  # P(C_k) * P(x|C_k)\n",
    "    \n",
    "    label = np.argmax(label_prob)  # pred label\n",
    "    index_to_label = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci_Tech\"}\n",
    "    return index_to_label[label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Comets, Asteroids and Planets around a Nearby Star (SPACE.com) SPACE.com - A nearby star thought to harbor comets and asteroids now appears to be home to planets, too. The presumed worlds are smaller than Jupiter and could be as tiny as Pluto, new observations suggest.'\"classification copy.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci_Tech\n"
     ]
    }
   ],
   "source": [
    "print(inference(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
